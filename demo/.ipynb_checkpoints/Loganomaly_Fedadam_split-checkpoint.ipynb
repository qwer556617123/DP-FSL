{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:02.620274Z",
     "start_time": "2022-01-14T09:50:02.618500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set GPU number. This might work for several framework such as PyTorch, TensorFlow, Keras\n",
    "gpu_id = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpu_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:02.622630Z",
     "start_time": "2022-01-14T09:50:02.621122Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:03.506190Z",
     "start_time": "2022-01-14T09:50:02.623464Z"
    }
   },
   "outputs": [],
   "source": [
    "# sys.path.append('../../loglizer')\n",
    "    \n",
    "# from loglizer import dataloader\n",
    "# from loglizer.preprocessing import Vectorizer, Iterator\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "import time\n",
    "import copy\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:03.519312Z",
     "start_time": "2022-01-14T09:50:03.507387Z"
    }
   },
   "outputs": [],
   "source": [
    "# struct_log = '../data/HDFS/HDFS.log_structured.csv' # The structured log file\n",
    "\n",
    "#初始參數\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.mode = 'fedadam'  #FedBN,FedAvg,fedprox,fedopt,fedadagrad,fedadam 待加入scoffold\n",
    "        #所有方法都有使用Fedbn\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.99 #b1,b2 for adaptive opt\n",
    "        self.tau = 1e-2\n",
    "        self.batch = 32\n",
    "        self.lr = 1e-2\n",
    "        self.server_lr = 1e-2\n",
    "        self.server_momentum = 0.5\n",
    "        self.client_momentum = 0.9 # 0 is fedavg others is fedavgm\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.client_num = 5\n",
    "        self.wk_iters = 1 # training\n",
    "        self.num_workers = 2 # dataloader\n",
    "        self.mu = 1e-3\n",
    "        self.iters = 500 #epochs\n",
    "        self.local_epoches = 1\n",
    "        self.server_opt = 'sgd'\n",
    "        self.percent = 1\n",
    "#         self.early_stop = 500\n",
    "        self.window_size = 10\n",
    "        self.train_ratio = 0.5\n",
    "        self.split_type='sequential' # sequential, uniform\n",
    "\n",
    "        \n",
    "args = Parser()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed) \n",
    "random.seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:03.531653Z",
     "start_time": "2022-01-14T09:50:03.520463Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from logdeep.models.lstm import deeplog, loganomaly, robustlog\n",
    "from logdeep.tools.predict import Predicter\n",
    "from logdeep.tools.train import Trainer\n",
    "from logdeep.tools.utils import *\n",
    "\n",
    "# Config Parameters\n",
    "\n",
    "options = dict()\n",
    "options['data_dir'] = '../data/'\n",
    "options['window_size'] = 10\n",
    "options['device'] = 0\n",
    "\n",
    "# Smaple\n",
    "options['sample'] = \"sliding_window\"\n",
    "options['window_size'] = 10  # if fix_window\n",
    "\n",
    "# Features\n",
    "options['sequentials'] = True\n",
    "options['quantitatives'] = False\n",
    "options['semantics'] = False\n",
    "options['feature_num'] = sum(\n",
    "    [options['sequentials'], options['quantitatives'], options['semantics']])\n",
    "\n",
    "# Model\n",
    "options['input_size'] = 1\n",
    "options['hidden_size'] = 64\n",
    "options['num_layers'] = 2\n",
    "options['num_classes'] = 28\n",
    "\n",
    "# Train\n",
    "options['batch_size'] = 2048\n",
    "options['accumulation_step'] = 1\n",
    "\n",
    "options['optimizer'] = 'adam'\n",
    "options['lr'] = 0.001\n",
    "options['max_epoch'] = 1\n",
    "options['lr_step'] = (300, 350)\n",
    "options['lr_decay_ratio'] = 0.1\n",
    "\n",
    "options['resume_path'] = None\n",
    "options['model_name'] = \"deeplog\"\n",
    "options['save_dir'] = \"../result/deeplog/\"\n",
    "\n",
    "# Predict\n",
    "options['model_path'] = \"../result/deeplog/deeplog_last.pth\"\n",
    "options['num_candidates'] = 9\n",
    "\n",
    "seed_everything(seed=1234)\n",
    "\n",
    "\n",
    "def train():\n",
    "    trainer = Trainer(Model, options)\n",
    "    trainer.start_train()\n",
    "\n",
    "\n",
    "def predict():\n",
    "    predicter = Predicter(Model, options)\n",
    "    predicter.predict_unsupervised()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:03.533987Z",
     "start_time": "2022-01-14T09:50:03.532609Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(struct_log)\n",
    "# print(df.shape)\n",
    "\n",
    "# for i in range(args.client_num):\n",
    "#     bound = int(df.shape[0]/args.client_num)\n",
    "#     ddf = df[i* bound : (i+1)* bound]\n",
    "#     ddf.to_csv(\"../../loglizer/data/client_alldata/client_\"+str(i)+\".csv\", index=None)\n",
    "#     print('Client',i, ddf.shape)\n",
    "#     del ddf\n",
    "# del df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:03.544521Z",
     "start_time": "2022-01-14T09:50:03.534862Z"
    }
   },
   "outputs": [],
   "source": [
    "class deeplog(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_keys):\n",
    "        super(deeplog, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size,\n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_keys)\n",
    "\n",
    "    def forward(self, features, device):\n",
    "        input0 = features[0]\n",
    "        h0 = torch.zeros(self.num_layers, input0.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, input0.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(input0, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class loganomaly(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_keys):\n",
    "        super(loganomaly, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm0 = nn.LSTM(input_size,\n",
    "                             hidden_size,\n",
    "                             num_layers,\n",
    "                             batch_first=True)\n",
    "        self.lstm1 = nn.LSTM(input_size,\n",
    "                             hidden_size,\n",
    "                             num_layers,\n",
    "                             batch_first=True)\n",
    "        self.fc = nn.Linear(2 * hidden_size, num_keys)\n",
    "        self.attention_size = self.hidden_size\n",
    "\n",
    "        self.w_omega = Variable(\n",
    "            torch.zeros(self.hidden_size, self.attention_size))\n",
    "        self.u_omega = Variable(torch.zeros(self.attention_size))\n",
    "\n",
    "        self.sequence_length = 28\n",
    "\n",
    "    def attention_net(self, lstm_output):\n",
    "        output_reshape = torch.Tensor.reshape(lstm_output,\n",
    "                                              [-1, self.hidden_size])\n",
    "        attn_tanh = torch.tanh(torch.mm(output_reshape, self.w_omega))\n",
    "        attn_hidden_layer = torch.mm(\n",
    "            attn_tanh, torch.Tensor.reshape(self.u_omega, [-1, 1]))\n",
    "        exps = torch.Tensor.reshape(torch.exp(attn_hidden_layer),\n",
    "                                    [-1, self.sequence_length])\n",
    "        alphas = exps / torch.Tensor.reshape(torch.sum(exps, 1), [-1, 1])\n",
    "        alphas_reshape = torch.Tensor.reshape(alphas,\n",
    "                                              [-1, self.sequence_length, 1])\n",
    "        state = lstm_output\n",
    "        attn_output = torch.sum(state * alphas_reshape, 1)\n",
    "        return attn_output\n",
    "\n",
    "    def forward(self, features, device):\n",
    "        input0, input1 = features[0], features[1]\n",
    "\n",
    "        h0_0 = torch.zeros(self.num_layers, input0.size(0),\n",
    "                           self.hidden_size).to(device)\n",
    "        c0_0 = torch.zeros(self.num_layers, input0.size(0),\n",
    "                           self.hidden_size).to(device)\n",
    "\n",
    "        out0, _ = self.lstm0(input0, (h0_0, c0_0))\n",
    "\n",
    "        h0_1 = torch.zeros(self.num_layers, input1.size(0),\n",
    "                           self.hidden_size).to(device)\n",
    "        c0_1 = torch.zeros(self.num_layers, input1.size(0),\n",
    "                           self.hidden_size).to(device)\n",
    "\n",
    "        out1, _ = self.lstm1(input1, (h0_1, c0_1))\n",
    "        multi_out = torch.cat((out0[:, -1, :], out1[:, -1, :]), -1)\n",
    "        out = self.fc(multi_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class robustlog(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_keys):\n",
    "        super(robustlog, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size,\n",
    "                            hidden_size,\n",
    "                            num_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_keys)\n",
    "\n",
    "    def forward(self, features, device):\n",
    "        input0 = features[0]\n",
    "        h0 = torch.zeros(self.num_layers, input0.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, input0.size(0),\n",
    "                         self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(input0, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:03.547378Z",
     "start_time": "2022-01-14T09:50:03.545791Z"
    }
   },
   "outputs": [],
   "source": [
    "# def datasplit(args, gpu_id = 1):\n",
    "#     train_loaders = []\n",
    "#     test_loaders = []\n",
    "#     hidden_size = 32\n",
    "#     num_directions = 2\n",
    "#     topk = 5\n",
    "    \n",
    "#     for i in range(args.client_num):\n",
    "#         struct_log = '../../loglizer/data/client_data/client_'+str(i)+\".csv\"\n",
    "#         label_file = '../../loglizer/data/HDFS/anomaly_label.csv' # The anomaly label file\n",
    "        \n",
    "#         (x_train, window_y_train, y_train), (x_test, window_y_test, y_test) = dataloader.load_HDFS(i, struct_log, label_file = label_file, window='session', window_size=args.window_size, train_ratio=args.train_ratio, split_type=args.split_type)\n",
    "#         feature_extractor = Vectorizer()\n",
    "#         train_dataset = feature_extractor.fit_transform(x_train, window_y_train, y_train)\n",
    "#         test_dataset = feature_extractor.transform(x_test, window_y_test, y_test)\n",
    "#         train_loader = Iterator(train_dataset, batch_size=args.batch, shuffle=True, num_workers=args.num_workers).iter\n",
    "#         test_loader = Iterator(test_dataset, batch_size=args.batch, shuffle=False, num_workers=args.num_workers).iter\n",
    "#         train_loaders.append(train_loader)\n",
    "#         test_loaders.append(test_loader)\n",
    "        \n",
    "#     server_model = DeepLog(num_labels=feature_extractor.num_labels, hidden_size=hidden_size, num_directions=num_directions, topk=topk).to(device)\n",
    "    \n",
    "#     return train_loaders, test_loaders, server_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:03.549867Z",
     "start_time": "2022-01-14T09:50:03.548296Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_loaders, test_loaders, server_model = datasplit(args, gpu_id = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T09:50:03.559293Z",
     "start_time": "2022-01-14T09:50:03.550725Z"
    }
   },
   "outputs": [],
   "source": [
    "def communication(args, server_model, models, client_weights, v, grad):\n",
    "    if args.mode.lower() == 'fedbn':\n",
    "        with torch.no_grad():\n",
    "            for key in server_model.state_dict().keys():\n",
    "                if 'bn' not in key: #非BN層都去交換\n",
    "                    temp = torch.zeros_like(server_model.state_dict()[key], dtype=torch.float32)\n",
    "                    for client_idx in range(client_num):\n",
    "                        temp += client_weights[client_idx] * models[client_idx].state_dict()[key]\n",
    "                    server_model.state_dict()[key].data.copy_(temp)\n",
    "                    for client_idx in range(client_num):\n",
    "                        models[client_idx].state_dict()[key].data.copy_(server_model.state_dict()[key])\n",
    "    \n",
    "    elif args.mode.lower() == 'fedadagrad':\n",
    "        with torch.no_grad():\n",
    "            for key, param in server_model.named_parameters():\n",
    "                temp = torch.zeros_like(server_model.state_dict()[key])\n",
    "                for client_idx in range(len(client_weights)):\n",
    "                    temp = temp + client_weights[client_idx] * models[client_idx].state_dict()[key]                          \n",
    "                param.grad = temp - param.data              \n",
    "                param.grad = torch.mul(grad[key], args.beta_1) + torch.mul(param.grad, 1-args.beta_1)\n",
    "                grad[key] = param.grad\n",
    "                v[key] = v[key] + param.grad**2\n",
    "                param.data = param.data + torch.mul(torch.div(param.grad, torch.add(torch.sqrt(v[key]),args.tau)), args.server_lr) \n",
    "\n",
    "                for client_idx in range(len(client_weights)):\n",
    "                    models[client_idx].state_dict()[key].data.copy_(server_model.state_dict()[key])            \n",
    "    \n",
    "    elif args.mode.lower() == 'fedadam':\n",
    "        with torch.no_grad():\n",
    "            for key, param in server_model.named_parameters():                \n",
    "                temp = torch.zeros_like(server_model.state_dict()[key])\n",
    "                for client_idx in range(len(client_weights)):\n",
    "                    temp += client_weights[client_idx] * models[client_idx].state_dict()[key]                         \n",
    "                param.grad = temp - param.data              \n",
    "                param.grad = torch.mul(grad[key], args.beta_1) + torch.mul(param.grad, 1-args.beta_1) \n",
    "                grad[key] = param.grad                \n",
    "                v[key] = torch.mul(v[key], args.beta_2) + torch.mul(param.grad**2, 1-args.beta_2)\n",
    "                param.data = param.data + torch.mul(torch.div(param.grad, torch.add(torch.sqrt(v[key]),args.tau)), args.server_lr)\n",
    "                \n",
    "                for client_idx in range(len(client_weights)):\n",
    "                    models[client_idx].state_dict()[key].data.copy_(server_model.state_dict()[key])\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for key in server_model.state_dict().keys():#遇到BN層就直接拿第一個client參數使用\n",
    "                # num_batches_tracked is a non trainable LongTensor and\n",
    "                # num_batches_tracked are the same for all clients for the given datasets\n",
    "                if 'num_batches_tracked' in key:\n",
    "                    server_model.state_dict()[key].data.copy_(models[0].state_dict()[key])\n",
    "                else:\n",
    "                    temp = torch.zeros_like(server_model.state_dict()[key]).to(device)\n",
    "                    for client_idx in range(len(client_weights)):\n",
    "                        temp += client_weights[client_idx] * models[client_idx].state_dict()[key]                        \n",
    "                    server_model.state_dict()[key].data.copy_(temp)#weight傳給server\n",
    "                    for client_idx in range(len(client_weights)):\n",
    "                        models[client_idx].state_dict()[key].data.copy_(server_model.state_dict()[key])        \n",
    "    return server_model, models, v, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T09:50:02.132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Train epoch 0 ============\n",
      "Client  0\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9420\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1058/1058 [00:00<00:00, 3310.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1058\n",
      "Find 9420 train logs, 1058 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:50:17 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31278: 100%|██████████| 5/5 [00:00<00:00, 46.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:50:17 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 100.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3099191188812256\n",
      "Client  1\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9080\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1046/1046 [00:00<00:00, 3421.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1046\n",
      "Find 9080 train logs, 1046 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:50:28 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31121: 100%|██████████| 5/5 [00:00<00:00, 55.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:50:29 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 100.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.310542106628418\n",
      "Client  2\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9485\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [00:00<00:00, 3572.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1051\n",
      "Find 9485 train logs, 1051 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:50:40 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31409: 100%|██████████| 5/5 [00:00<00:00, 53.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:50:40 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 100.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.311701774597168\n",
      "Client  3\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9267\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1057/1057 [00:00<00:00, 3527.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1057\n",
      "Find 9267 train logs, 1057 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:50:52 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31225: 100%|██████████| 5/5 [00:00<00:00, 53.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:50:52 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 91.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3112874031066895\n",
      "Client  4\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9323\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [00:00<00:00, 3430.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1044\n",
      "Find 9323 train logs, 1044 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:51:04 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31194: 100%|██████████| 5/5 [00:00<00:00, 53.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:51:04 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 101.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3155789375305176\n",
      "============ Test epoch 0 ============\n",
      "Client  0\n",
      "Number of sessions(hdfs_test_normal): 5813\n",
      "Number of sessions(hdfs_test_abnormal): 1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5813/5813 [00:13<00:00, 418.90it/s]\n",
      "100%|██████████| 1136/1136 [00:02<00:00, 450.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110641, false negative (FN): 1, Precision: 2.952%, Recall: 99.970%, F1-measure: 5.736%\n",
      "Finished Predicting\n",
      "elapsed_time: 16.402056455612183\n",
      "Client  1\n",
      "Number of sessions(hdfs_test_normal): 5574\n",
      "Number of sessions(hdfs_test_abnormal): 1139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5574/5574 [00:13<00:00, 408.37it/s]\n",
      "100%|██████████| 1139/1139 [00:02<00:00, 428.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110645, false negative (FN): 3, Precision: 2.951%, Recall: 99.911%, F1-measure: 5.732%\n",
      "Finished Predicting\n",
      "elapsed_time: 16.311716556549072\n",
      "Client  2\n",
      "Number of sessions(hdfs_test_normal): 5730\n",
      "Number of sessions(hdfs_test_abnormal): 1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5730/5730 [00:13<00:00, 416.77it/s]\n",
      "100%|██████████| 1118/1118 [00:02<00:00, 430.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110647, false negative (FN): 1, Precision: 2.952%, Recall: 99.970%, F1-measure: 5.735%\n",
      "Finished Predicting\n",
      "elapsed_time: 16.345041513442993\n",
      "Client  3\n",
      "Number of sessions(hdfs_test_normal): 5747\n",
      "Number of sessions(hdfs_test_abnormal): 1159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5747/5747 [00:13<00:00, 411.56it/s]\n",
      "100%|██████████| 1159/1159 [00:02<00:00, 494.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110653, false negative (FN): 0, Precision: 2.953%, Recall: 100.000%, F1-measure: 5.737%\n",
      "Finished Predicting\n",
      "elapsed_time: 16.308897733688354\n",
      "Client  4\n",
      "Number of sessions(hdfs_test_normal): 5623\n",
      "Number of sessions(hdfs_test_abnormal): 1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5623/5623 [00:09<00:00, 563.10it/s]\n",
      "100%|██████████| 1121/1121 [00:01<00:00, 788.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110650, false negative (FN): 0, Precision: 2.953%, Recall: 100.000%, F1-measure: 5.737%\n",
      "Finished Predicting\n",
      "elapsed_time: 11.409134864807129\n",
      "============ Train epoch 1 ============\n",
      "Client  0\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9420\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1058/1058 [00:00<00:00, 3280.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1058\n",
      "Find 9420 train logs, 1058 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:52:36 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31264: 100%|██████████| 5/5 [00:00<00:00, 46.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:52:36 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 69.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.30757212638855\n",
      "Client  1\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9080\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1046/1046 [00:00<00:00, 3326.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1046\n",
      "Find 9080 train logs, 1046 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:52:48 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31095: 100%|██████████| 5/5 [00:00<00:00, 50.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:52:48 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 97.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3108527660369873\n",
      "Client  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9485\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [00:00<00:00, 3293.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1051\n",
      "Find 9485 train logs, 1051 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:53:00 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31356: 100%|██████████| 5/5 [00:00<00:00, 44.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:53:00 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 68.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.305565118789673\n",
      "Client  3\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9267\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1057/1057 [00:00<00:00, 3561.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1057\n",
      "Find 9267 train logs, 1057 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:53:12 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31170: 100%|██████████| 5/5 [00:00<00:00, 54.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:53:12 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 104.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3115875720977783\n",
      "Client  4\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9323\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [00:00<00:00, 3563.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1044\n",
      "Find 9323 train logs, 1044 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:53:24 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31136: 100%|██████████| 5/5 [00:00<00:00, 42.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:53:24 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 88.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.307098865509033\n",
      "============ Test epoch 1 ============\n",
      "Client  0\n",
      "Number of sessions(hdfs_test_normal): 5813\n",
      "Number of sessions(hdfs_test_abnormal): 1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5813/5813 [00:14<00:00, 412.85it/s]\n",
      "100%|██████████| 1136/1136 [00:02<00:00, 490.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110641, false negative (FN): 1, Precision: 2.952%, Recall: 99.970%, F1-measure: 5.736%\n",
      "Finished Predicting\n",
      "elapsed_time: 16.399830102920532\n",
      "Client  1\n",
      "Number of sessions(hdfs_test_normal): 5574\n",
      "Number of sessions(hdfs_test_abnormal): 1139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5574/5574 [00:13<00:00, 409.20it/s]\n",
      "100%|██████████| 1139/1139 [00:02<00:00, 428.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110645, false negative (FN): 3, Precision: 2.951%, Recall: 99.911%, F1-measure: 5.732%\n",
      "Finished Predicting\n",
      "elapsed_time: 16.27965474128723\n",
      "Client  2\n",
      "Number of sessions(hdfs_test_normal): 5730\n",
      "Number of sessions(hdfs_test_abnormal): 1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5730/5730 [00:13<00:00, 412.93it/s]\n",
      "100%|██████████| 1118/1118 [00:02<00:00, 457.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110647, false negative (FN): 1, Precision: 2.952%, Recall: 99.970%, F1-measure: 5.735%\n",
      "Finished Predicting\n",
      "elapsed_time: 16.320284366607666\n",
      "Client  3\n",
      "Number of sessions(hdfs_test_normal): 5747\n",
      "Number of sessions(hdfs_test_abnormal): 1159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5747/5747 [00:13<00:00, 411.88it/s]\n",
      "100%|██████████| 1159/1159 [00:02<00:00, 450.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110653, false negative (FN): 0, Precision: 2.953%, Recall: 100.000%, F1-measure: 5.737%\n",
      "Finished Predicting\n",
      "elapsed_time: 16.529722452163696\n",
      "Client  4\n",
      "Number of sessions(hdfs_test_normal): 5623\n",
      "Number of sessions(hdfs_test_abnormal): 1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5623/5623 [00:09<00:00, 580.61it/s]\n",
      "100%|██████████| 1121/1121 [00:01<00:00, 607.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110650, false negative (FN): 0, Precision: 2.953%, Recall: 100.000%, F1-measure: 5.737%\n",
      "Finished Predicting\n",
      "elapsed_time: 11.532071352005005\n",
      "============ Train epoch 2 ============\n",
      "Client  0\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9420\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1058/1058 [00:00<00:00, 3432.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1058\n",
      "Find 9420 train logs, 1058 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:54:56 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31183: 100%|██████████| 5/5 [00:00<00:00, 45.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:54:56 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 91.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3079469203948975\n",
      "Client  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9080\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1046/1046 [00:00<00:00, 3488.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1046\n",
      "Find 9080 train logs, 1046 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:55:08 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31063: 100%|██████████| 5/5 [00:00<00:00, 55.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:55:08 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 98.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3064002990722656\n",
      "Client  2\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9485\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [00:00<00:00, 3361.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1051\n",
      "Find 9485 train logs, 1051 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:55:20 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31309: 100%|██████████| 5/5 [00:00<00:00, 54.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:55:20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 99.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3096978664398193\n",
      "Client  3\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9267\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1057/1057 [00:00<00:00, 1969.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1057\n",
      "Find 9267 train logs, 1057 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:55:32 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31109: 100%|██████████| 5/5 [00:00<00:00, 51.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:55:33 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 93.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3094630241394043\n",
      "Client  4\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9323\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [00:00<00:00, 3169.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1044\n",
      "Find 9323 train logs, 1044 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:55:44 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31124: 100%|██████████| 5/5 [00:00<00:00, 53.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:55:45 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 105.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.313577175140381\n",
      "============ Test epoch 2 ============\n",
      "Client  0\n",
      "Number of sessions(hdfs_test_normal): 5813\n",
      "Number of sessions(hdfs_test_abnormal): 1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5813/5813 [00:07<00:00, 777.85it/s]\n",
      "100%|██████████| 1136/1136 [00:01<00:00, 821.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110641, false negative (FN): 1, Precision: 2.952%, Recall: 99.970%, F1-measure: 5.736%\n",
      "Finished Predicting\n",
      "elapsed_time: 8.859044075012207\n",
      "Client  1\n",
      "Number of sessions(hdfs_test_normal): 5574\n",
      "Number of sessions(hdfs_test_abnormal): 1139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5574/5574 [00:07<00:00, 778.51it/s]\n",
      "100%|██████████| 1139/1139 [00:01<00:00, 791.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110645, false negative (FN): 3, Precision: 2.951%, Recall: 99.911%, F1-measure: 5.732%\n",
      "Finished Predicting\n",
      "elapsed_time: 8.60227108001709\n",
      "Client  2\n",
      "Number of sessions(hdfs_test_normal): 5730\n",
      "Number of sessions(hdfs_test_abnormal): 1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5730/5730 [00:07<00:00, 778.57it/s]\n",
      "100%|██████████| 1118/1118 [00:01<00:00, 793.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110647, false negative (FN): 1, Precision: 2.952%, Recall: 99.970%, F1-measure: 5.735%\n",
      "Finished Predicting\n",
      "elapsed_time: 8.771571636199951\n",
      "Client  3\n",
      "Number of sessions(hdfs_test_normal): 5747\n",
      "Number of sessions(hdfs_test_abnormal): 1159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5747/5747 [00:09<00:00, 624.53it/s]\n",
      "100%|██████████| 1159/1159 [00:02<00:00, 542.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110653, false negative (FN): 0, Precision: 2.953%, Recall: 100.000%, F1-measure: 5.737%\n",
      "Finished Predicting\n",
      "elapsed_time: 11.342010259628296\n",
      "Client  4\n",
      "Number of sessions(hdfs_test_normal): 5623\n",
      "Number of sessions(hdfs_test_abnormal): 1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5623/5623 [00:10<00:00, 529.54it/s]\n",
      "100%|██████████| 1121/1121 [00:02<00:00, 549.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110650, false negative (FN): 0, Precision: 2.953%, Recall: 100.000%, F1-measure: 5.737%\n",
      "Finished Predicting\n",
      "elapsed_time: 12.660226106643677\n",
      "============ Train epoch 3 ============\n",
      "Client  0\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9420\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1058/1058 [00:00<00:00, 3473.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1058\n",
      "Find 9420 train logs, 1058 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:56:50 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31095: 100%|██████████| 5/5 [00:00<00:00, 51.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:56:50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 98.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3114418983459473\n",
      "Client  1\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9080\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1046/1046 [00:00<00:00, 3440.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1046\n",
      "Find 9080 train logs, 1046 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:57:02 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31000: 100%|██████████| 5/5 [00:00<00:00, 55.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:57:02 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 95.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3111865520477295\n",
      "Client  2\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9485\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1051/1051 [00:00<00:00, 3436.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1051\n",
      "Find 9485 train logs, 1051 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:57:14 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31239: 100%|██████████| 5/5 [00:00<00:00, 42.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:57:14 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 97.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3044581413269043\n",
      "Client  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9267\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1057/1057 [00:00<00:00, 3596.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1057\n",
      "Find 9267 train logs, 1057 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:57:26 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31056: 100%|██████████| 5/5 [00:00<00:00, 55.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:57:26 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 85.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3066840171813965\n",
      "Client  4\n",
      "File ../data/hdfs/hdfs_train, number of sessions 971\n",
      "File ../data/hdfs/hdfs_train, number of seqs 9323\n",
      "sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [00:00<00:00, 1988.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/hdfs/hdfs_test_normal, number of sessions 110673\n",
      "File ../data/hdfs/hdfs_test_normal, number of seqs 1044\n",
      "Find 9323 train logs, 1044 validation logs\n",
      "Train batch size 2048 ,Validation batch size 2048\n",
      "Starting epoch: 0 | phase: train | ⏰: 17:57:38 | Learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 3.31004: 100%|██████████| 5/5 [00:00<00:00, 47.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: valid | ⏰: 17:57:38 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 100%|██████████| 1/1 [00:00<00:00, 100.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.3082187175750732\n",
      "============ Test epoch 3 ============\n",
      "Client  0\n",
      "Number of sessions(hdfs_test_normal): 5813\n",
      "Number of sessions(hdfs_test_abnormal): 1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5813/5813 [00:10<00:00, 536.40it/s]\n",
      "100%|██████████| 1136/1136 [00:01<00:00, 812.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110639, false negative (FN): 1, Precision: 2.953%, Recall: 99.970%, F1-measure: 5.736%\n",
      "Finished Predicting\n",
      "elapsed_time: 12.238496780395508\n",
      "Client  1\n",
      "Number of sessions(hdfs_test_normal): 5574\n",
      "Number of sessions(hdfs_test_abnormal): 1139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5574/5574 [00:07<00:00, 775.81it/s]\n",
      "100%|██████████| 1139/1139 [00:01<00:00, 786.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110645, false negative (FN): 4, Precision: 2.950%, Recall: 99.881%, F1-measure: 5.730%\n",
      "Finished Predicting\n",
      "elapsed_time: 8.635941982269287\n",
      "Client  2\n",
      "Number of sessions(hdfs_test_normal): 5730\n",
      "Number of sessions(hdfs_test_abnormal): 1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5730/5730 [00:07<00:00, 772.76it/s]\n",
      "100%|██████████| 1118/1118 [00:01<00:00, 790.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110643, false negative (FN): 1, Precision: 2.952%, Recall: 99.970%, F1-measure: 5.735%\n",
      "Finished Predicting\n",
      "elapsed_time: 8.831627130508423\n",
      "Client  3\n",
      "Number of sessions(hdfs_test_normal): 5747\n",
      "Number of sessions(hdfs_test_abnormal): 1159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5747/5747 [00:07<00:00, 782.60it/s]\n",
      "100%|██████████| 1159/1159 [00:01<00:00, 804.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive (FP): 110653, false negative (FN): 1, Precision: 2.952%, Recall: 99.970%, F1-measure: 5.735%\n",
      "Finished Predicting\n",
      "elapsed_time: 8.787116527557373\n",
      "Client  4\n",
      "Number of sessions(hdfs_test_normal): 5623\n",
      "Number of sessions(hdfs_test_abnormal): 1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2490/5623 [00:03<00:04, 766.30it/s]"
     ]
    }
   ],
   "source": [
    "# # setup model\n",
    "server_model = loganomaly(input_size=options['input_size'],\n",
    "                    hidden_size=options['hidden_size'],\n",
    "                    num_layers=options['num_layers'],\n",
    "                    num_keys=options['num_classes']).to(device)\n",
    "# for adaptive velocity\n",
    "v = {}\n",
    "grad = {}\n",
    "for key in server_model.state_dict().keys():\n",
    "    v[key] = torch.add(torch.zeros_like(server_model.state_dict()[key],dtype=torch.float32),args.tau**2)\n",
    "    grad[key] = torch.zeros_like(server_model.state_dict()[key],dtype=torch.float32)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "# name of each datasets\n",
    "datasets = ['Client'+str(i) for i in range(args.client_num)]\n",
    "# federated client number\n",
    "client_num = len(datasets)\n",
    "client_weights = [1/client_num for i in range(client_num)]\n",
    "# each local client model\n",
    "models = [copy.deepcopy(server_model).to(device) for idx in range(client_num)]\n",
    "\n",
    "best_epoch = 0\n",
    "best_f1 = [0. for j in range(client_num)] \n",
    "start_iter = 0\n",
    "\n",
    "result_train_loss = {}\n",
    "result_train_sec_acc = {}\n",
    "result_train_win_acc = {}\n",
    "result_train_f1 = {}\n",
    "result_train_recall = {}\n",
    "result_train_precision = {}\n",
    "\n",
    "result_test_sec_acc = {}\n",
    "result_test_win_acc = {}\n",
    "result_test_f1 = {}\n",
    "result_test_recall = {}\n",
    "result_test_precision = {}\n",
    "\n",
    "for i in range(args.client_num):    \n",
    "    result_train_loss[i] = []\n",
    "    result_train_sec_acc[i] = []\n",
    "    result_train_win_acc[i] = []\n",
    "    result_train_f1[i] = []\n",
    "    result_train_recall[i] = []\n",
    "    result_train_precision[i] = []\n",
    "\n",
    "    result_test_sec_acc[i] = []\n",
    "    result_test_win_acc[i] = []\n",
    "    result_test_f1[i] = []\n",
    "    result_test_recall[i] = []\n",
    "    result_test_precision[i] = []\n",
    "\n",
    "# Start training\n",
    "for a_iter in range(start_iter, args.iters):\n",
    "#     optimizers = [optim.SGD(params = models[idx].parameters(), lr=args.lr, momentum = args.client_momentum) for idx in range(client_num)]\n",
    "    for wi in range(args.wk_iters):\n",
    "        print(\"============ Train epoch {} ============\".format(wi + a_iter * args.wk_iters))\n",
    "        for client_idx, model in enumerate(models):\n",
    "            print('Client ', client_idx)\n",
    "            trainer = Trainer(model, options, args.client_num, client_idx)\n",
    "            trainer.start_train()\n",
    "            \n",
    "#             train_loss = train(model, train_loaders[client_idx], optimizers[client_idx], device, args.local_epoches)\n",
    "#             result_train_loss[client_idx].append(train_loss)\n",
    "    with torch.no_grad():\n",
    "        # aggregation\n",
    "        server_model, models, v, grad = communication(args, server_model, models, client_weights, v, grad )\n",
    "        # Report loss after aggregation\n",
    "\n",
    "#         for client_idx, model in enumerate(models):\n",
    "#             tr_metrics = test(model, train_loaders[client_idx], device)\n",
    "#             result_train_sec_acc[client_idx].append(tr_metrics['session_acc'])\n",
    "#             result_train_win_acc[client_idx].append(tr_metrics['window_acc'])\n",
    "#             result_train_f1[client_idx].append(tr_metrics['f1'])\n",
    "#             result_train_recall[client_idx].append(tr_metrics['recall'])\n",
    "#             result_train_precision[client_idx].append(tr_metrics['precision'])\n",
    "            \n",
    "#             print(' {:<8s}| Loss: {:.4f} | Session Acc: {:.4f}| Window Acc: {:.4f} | F1: {:.4f} | Recall: {:.4f} | Precision: {:.4f} '.format(datasets[client_idx], result_train_loss[client_idx][-1], result_train_sec_acc[client_idx][-1], result_train_win_acc[client_idx][-1],\n",
    "#                                                                                 result_train_f1[client_idx][-1], result_train_recall[client_idx][-1], result_train_precision[client_idx][-1])) \n",
    "          \n",
    "        print(\"============ Test epoch {} ============\".format(wi + a_iter * args.wk_iters))\n",
    "#         # Validation\n",
    "#         val_f1_list = [None for j in range(client_num)]\n",
    "        for client_idx, model in enumerate(models):\n",
    "            print('Client ', client_idx)\n",
    "            predicter = Predicter(model, options, args.client_num, client_idx)\n",
    "            predicter.predict_unsupervised()\n",
    "#             te_metrics = test(model, test_loaders[client_idx], device)\n",
    "#             val_f1_list[client_idx] = te_metrics['f1']\n",
    "#             result_test_sec_acc[client_idx].append(te_metrics['session_acc'])\n",
    "#             result_test_win_acc[client_idx].append(te_metrics['window_acc'])\n",
    "#             result_test_f1[client_idx].append(te_metrics['f1'])\n",
    "#             result_test_recall[client_idx].append(te_metrics['recall'])\n",
    "#             result_test_precision[client_idx].append(te_metrics['precision'])\n",
    "            \n",
    "#             print(' {:<8s}| Session Acc: {:.4f}| Window Acc: {:.4f} | F1: {:.4f} | Recall: {:.4f} | Precision: {:.4f} '.format(datasets[client_idx], result_test_sec_acc[client_idx][-1], result_test_win_acc[client_idx][-1],\n",
    "#                                                                                 result_test_f1[client_idx][-1], result_test_recall[client_idx][-1], result_test_precision[client_idx][-1])) \n",
    "            \n",
    "#         # Record best\n",
    "#         if np.mean(val_f1_list) > np.mean(best_f1):\n",
    "#             for client_idx in range(client_num):\n",
    "#                 best_f1[client_idx] = val_f1_list[client_idx]\n",
    "#                 best_epoch = a_iter\n",
    "#                 best_changed=True\n",
    "\n",
    "# for client_idx in range(client_num):\n",
    "#     print(' Best {:<10s}| Epoch:{} |Test F1: {:.4f}'.format(datasets[client_idx], best_epoch, best_f1[client_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T09:50:02.134Z"
    }
   },
   "outputs": [],
   "source": [
    "# # test on global model\n",
    "# if args.mode.lower() == 'fedbn':\n",
    "#     pass\n",
    "# else:\n",
    "#     for client_idx in range(client_num):\n",
    "#         models[client_idx].load_state_dict(server_model.state_dict())\n",
    "        \n",
    "# for test_idx, test_loader in enumerate(test_loaders):\n",
    "#     metrics = test(server_model, test_loader, device)\n",
    "#     print(' {:<8s}| Session Acc: {:.4f}| Window Acc: {:.4f} | F1: {:.4f} | Recall: {:.4f} | Precision: {:.4f} '.format(datasets[test_idx], metrics['session_acc'], metrics['window_acc'], metrics['f1'], metrics['recall'], metrics['precision']))\n",
    "\n",
    "predicter = Predicter(server_model, options, 1, 0)\n",
    "predicter.predict_unsupervised()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T09:50:02.136Z"
    }
   },
   "outputs": [],
   "source": [
    "# (pd.DataFrame.from_dict(data=result_train_loss, orient='index').to_csv('../data/result_all/Fedavg_232_1_train_loss.csv', header=False))\n",
    "\n",
    "# (pd.DataFrame.from_dict(data=result_train_sec_acc, orient='index').to_csv('../data/result_all/Fedavg_232_1_train_sec_acc.csv', header=False))\n",
    "# (pd.DataFrame.from_dict(data=result_train_win_acc, orient='index').to_csv('../data/result_all/Fedavg_232_1_train_win_acc.csv', header=False))\n",
    "# (pd.DataFrame.from_dict(data=result_train_f1, orient='index').to_csv('../data/result_all/Fedavg_232_1_train_f1.csv', header=False))\n",
    "# (pd.DataFrame.from_dict(data=result_train_recall, orient='index').to_csv('../data/result_all/Fedavg_232_1_train_recall.csv', header=False))\n",
    "# (pd.DataFrame.from_dict(data=result_train_precision, orient='index').to_csv('../data/result_all/Fedavg_232_1_train_precision.csv', header=False))\n",
    "\n",
    "# (pd.DataFrame.from_dict(data=result_test_sec_acc, orient='index').to_csv('../data/result_all/Fedavg_232_1_test_sec_acc.csv', header=False))\n",
    "# (pd.DataFrame.from_dict(data=result_test_win_acc, orient='index').to_csv('../data/result_all/Fedavg_232_1_test_win_acc.csv', header=False))\n",
    "# (pd.DataFrame.from_dict(data=result_test_f1, orient='index').to_csv('../data/result_all/Fedavg_232_1_test_f1.csv', header=False))\n",
    "# (pd.DataFrame.from_dict(data=result_test_recall, orient='index').to_csv('../data/result_all/Fedavg_232_1_test_recall.csv', header=False))\n",
    "# (pd.DataFrame.from_dict(data=result_test_precision, orient='index').to_csv('../data/result_all/Fedavg_232_1_test_precision.csv', header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T09:50:02.138Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_train_loss[i], label = datasets[i])\n",
    "# plt.title('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_train_sec_acc[i], label = datasets[i])\n",
    "# plt.title('Session Acc')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_train_win_acc[i], label = datasets[i])\n",
    "# plt.title('Window Acc')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_train_f1[i], label = datasets[i])\n",
    "# plt.title('F1 Score')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_train_recall[i], label = datasets[i])\n",
    "# plt.title('Recall')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_train_precision[i], label = datasets[i])\n",
    "# plt.title('Precison')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T09:50:02.140Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_test_sec_acc[i], label = datasets[i])\n",
    "# plt.title('Session Acc')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_test_win_acc[i], label = datasets[i])\n",
    "# plt.title('Window Acc')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_test_f1[i], label = datasets[i])\n",
    "# plt.title('F1 Score')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_test_recall[i], label = datasets[i])\n",
    "# plt.title('Recall')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(client_num):\n",
    "#     plt.plot(np.arange(args.iters), result_test_precision[i], label = datasets[i])\n",
    "# plt.title('Precison')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T09:50:02.142Z"
    }
   },
   "outputs": [],
   "source": [
    "# result_record_train = pd.DataFrame.from_dict(data=result_train_f1, orient='index')\n",
    "# result_record_train = np.mean(np.array(result_record_train), axis = 0)\n",
    "# result_record_train = result_record_train.reshape(-1)\n",
    "\n",
    "# result_record_test = pd.DataFrame.from_dict(data=result_test_f1, orient='index')\n",
    "# result_record_test = np.mean(np.array(result_record_test), axis = 0)\n",
    "# result_record_test = result_record_test.reshape(-1)\n",
    "\n",
    "# plt.plot(np.arange(args.iters), result_record_train, label = 'Train')\n",
    "# plt.plot(np.arange(args.iters), result_record_test, label = 'Test')\n",
    "# plt.title('Average F1 score')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T09:50:02.143Z"
    }
   },
   "outputs": [],
   "source": [
    "# result_record_train_loss = pd.DataFrame.from_dict(data=result_train_loss, orient='index')\n",
    "# result_record_train_loss = np.mean(np.array(result_record_train_loss), axis = 0)\n",
    "# result_record_train_loss = result_record_train_loss.reshape(-1)\n",
    "\n",
    "\n",
    "# plt.plot(np.arange(args.iters), result_record_train_loss, label = 'Train')\n",
    "# plt.title('Average train loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chi_env]",
   "language": "python",
   "name": "conda-env-chi_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
